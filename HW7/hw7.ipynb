{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Image Classification\n","\n","\n","**Authors:**  \n","Andrew Sen  \n","Neo Zhao\n","\n","In this notebook, we will use various types of neural networks to classify images of birds. The dataset, which can be found [here](https://www.kaggle.com/datasets/gpiosenka/100-bird-species), is a collection of over 75,000 images of birds of 450 different species. We will train each network to classify the images according to the species of bird depicted in each image."]},{"cell_type":"markdown","metadata":{},"source":["## Data Exploration"]},{"cell_type":"markdown","metadata":{},"source":["First, we'll have to read in the dataset."]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-12-05T01:35:46.260267Z","iopub.status.busy":"2022-12-05T01:35:46.259910Z","iopub.status.idle":"2022-12-05T01:35:46.613456Z","shell.execute_reply":"2022-12-05T01:35:46.612398Z","shell.execute_reply.started":"2022-12-05T01:35:46.260238Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","csvpath = r'../input/100-bird-species/birds.csv'\n","source_dir = r'../input/100-bird-species'\n","\n","df = pd.read_csv(csvpath)\n","df['filepaths'] = df['filepaths'].apply(lambda x: os.path.join(source_dir,x))   \n","classes = sorted(df['labels'].unique())\n","num_classes = len(classes)\n","img_size = (224, 224)\n","img_shape = (img_size[0], img_size[1], 3)"]},{"cell_type":"markdown","metadata":{},"source":["Let's make a graph showing the distribution of classes."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-12-04T07:22:00.942710Z","iopub.status.busy":"2022-12-04T07:22:00.942335Z","iopub.status.idle":"2022-12-04T07:22:06.609293Z","shell.execute_reply":"2022-12-04T07:22:06.608383Z","shell.execute_reply.started":"2022-12-04T07:22:00.942674Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<seaborn.axisgrid.FacetGrid at 0x7f640b8d8850>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 360x360 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["import seaborn as sb\n","\n","graph = sb.catplot(x='labels', kind='count', data=df)\n","graph.set(xticklabels=[])\n","graph.set(xlabel='')\n","graph"]},{"cell_type":"markdown","metadata":{},"source":["As we can see, the classes are about evenly distributed, with all of them having at least 100 associated images. Some classes have a much larger amount of images, but it shouldn't have too much of an effect on the final results."]},{"cell_type":"markdown","metadata":{},"source":["## Model Training\n","\n","First, we'll have to divide the data into train, test, and validate sets."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-12-05T01:35:51.440980Z","iopub.status.busy":"2022-12-05T01:35:51.440540Z","iopub.status.idle":"2022-12-05T01:39:18.785448Z","shell.execute_reply":"2022-12-05T01:39:18.784357Z","shell.execute_reply.started":"2022-12-05T01:35:51.440929Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 48182 validated image filenames belonging to 450 classes.\n","Found 11907 validated image filenames belonging to 450 classes.\n","Found 15037 validated image filenames belonging to 450 classes.\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras import models\n","from tensorflow.keras import layers\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","# setting random seeds\n","np.random.seed(1234)\n","\n","# train/test/validate split\n","i = np.random.rand(len(df)) < 0.8\n","train = df[i]\n","test = df[~i]\n","i = np.random.rand(len(train)) < 0.8\n","valid = train[~i]\n","train = train[i]\n","\n","batch_size = 30\n","gen = ImageDataGenerator()\n","\n","train_gen = gen.flow_from_dataframe(\n","    train,\n","    x_col = 'filepaths',\n","    y_col = 'labels',\n","    target_size = img_size,\n","    class_mode = 'categorical',\n","    color_mode = 'rgb',\n","    shuffle = True,\n","    batch_size = batch_size\n",")\n","\n","valid_gen = gen.flow_from_dataframe(\n","    valid,\n","    x_col = 'filepaths',\n","    y_col = 'labels',\n","    target_size = img_size,\n","    class_mode = 'categorical',\n","    color_mode = 'rgb',\n","    shuffle = True,\n","    batch_size = batch_size\n",")\n","\n","\n","test_gen = gen.flow_from_dataframe(\n","    test,\n","    x_col = 'filepaths',\n","    y_col = 'labels',\n","    target_size = img_size,\n","    class_mode = 'categorical',\n","    color_mode = 'rgb',\n","    shuffle = False,\n","    batch_size = batch_size\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Sequential Model"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-12-05T02:36:30.923989Z","iopub.status.busy":"2022-12-05T02:36:30.923621Z","iopub.status.idle":"2022-12-05T02:55:59.833710Z","shell.execute_reply":"2022-12-05T02:55:59.832577Z","shell.execute_reply.started":"2022-12-05T02:36:30.923959Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","1607/1607 [==============================] - 206s 128ms/step - loss: 110.1773 - accuracy: 0.0029 - val_loss: 6.1090 - val_accuracy: 0.0031\n","Epoch 2/5\n","1607/1607 [==============================] - 204s 127ms/step - loss: 6.1058 - accuracy: 0.0032 - val_loss: 6.1102 - val_accuracy: 0.0031\n","Epoch 3/5\n","1607/1607 [==============================] - 206s 128ms/step - loss: 6.1048 - accuracy: 0.0032 - val_loss: 6.1108 - val_accuracy: 0.0031\n","Epoch 4/5\n","1607/1607 [==============================] - 202s 126ms/step - loss: 6.1044 - accuracy: 0.0031 - val_loss: 6.1111 - val_accuracy: 0.0031\n","Epoch 5/5\n","1607/1607 [==============================] - 198s 123ms/step - loss: 6.1043 - accuracy: 0.0031 - val_loss: 6.1116 - val_accuracy: 0.0031\n","\n","accuracy:  0.0029926182084192327\n"]}],"source":["num_epochs = 5\n","\n","# define model topology\n","model_seq = models.Sequential()\n","model_seq.add(layers.Input(shape=img_shape))\n","model_seq.add(layers.Flatten())\n","model_seq.add(layers.Dense(256, activation='relu'))\n","model_seq.add(layers.Dense(256, activation='relu'))\n","model_seq.add(layers.Dense(256, activation='relu'))\n","model_seq.add(layers.Dense(num_classes, activation='softmax'))\n","\n","# train\n","model_seq.compile(\n","    optimizer = 'adam',\n","    loss = 'categorical_crossentropy',\n","    metrics = ['accuracy']\n",")\n","\n","# apply to test data\n","model_seq.fit(\n","    x = train_gen,\n","    epochs = num_epochs,\n","    validation_data = valid_gen,\n","    validation_steps = None,\n","    shuffle = False\n",")\n","\n","pred_seq = model_seq.predict(test_gen) # get predictions as label probabilities\n","pred_seq = np.argmax(pred_seq, axis=1) # get most likely label from probabilities\n","print('\\naccuracy: ', accuracy_score(test_gen.labels, pred_seq))"]},{"cell_type":"markdown","metadata":{},"source":["The accuracy of this model is extremely bad, being almost 0%. Although the model was only given 5 epochs for training, the accuracy had already plateaued by the second epoch. It's likely because the network is too simple to learn any valuable information about the dataset. Three dense layers with 256 nodes each do not make a complex enough network to process the large inputs."]},{"cell_type":"markdown","metadata":{},"source":["### Convolutional Neural Network"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-12-04T07:47:00.719437Z","iopub.status.busy":"2022-12-04T07:47:00.719058Z","iopub.status.idle":"2022-12-04T09:02:38.937210Z","shell.execute_reply":"2022-12-04T09:02:38.936071Z","shell.execute_reply.started":"2022-12-04T07:47:00.719402Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","1607/1607 [==============================] - 144s 89ms/step - loss: 5.2833 - accuracy: 0.0464 - val_loss: 5.2095 - val_accuracy: 0.0563\n","Epoch 2/30\n","1607/1607 [==============================] - 144s 89ms/step - loss: 4.5272 - accuracy: 0.1254 - val_loss: 4.4753 - val_accuracy: 0.1419\n","Epoch 3/30\n","1607/1607 [==============================] - 156s 97ms/step - loss: 4.0400 - accuracy: 0.1986 - val_loss: 4.1044 - val_accuracy: 0.1969\n","Epoch 4/30\n","1607/1607 [==============================] - 175s 109ms/step - loss: 3.6605 - accuracy: 0.2607 - val_loss: 3.8514 - val_accuracy: 0.2352\n","Epoch 5/30\n","1607/1607 [==============================] - 143s 89ms/step - loss: 3.3562 - accuracy: 0.3132 - val_loss: 3.4461 - val_accuracy: 0.2939\n","Epoch 6/30\n","1607/1607 [==============================] - 144s 90ms/step - loss: 3.0954 - accuracy: 0.3585 - val_loss: 3.9787 - val_accuracy: 0.2560\n","Epoch 7/30\n","1607/1607 [==============================] - 145s 90ms/step - loss: 2.8772 - accuracy: 0.3948 - val_loss: 5.1052 - val_accuracy: 0.1709\n","Epoch 8/30\n","1607/1607 [==============================] - 141s 88ms/step - loss: 2.6904 - accuracy: 0.4249 - val_loss: 3.2533 - val_accuracy: 0.3327\n","Epoch 9/30\n","1607/1607 [==============================] - 140s 87ms/step - loss: 2.5328 - accuracy: 0.4540 - val_loss: 2.8963 - val_accuracy: 0.4025\n","Epoch 10/30\n","1607/1607 [==============================] - 142s 88ms/step - loss: 2.4093 - accuracy: 0.4785 - val_loss: 3.7872 - val_accuracy: 0.2824\n","Epoch 11/30\n","1607/1607 [==============================] - 154s 96ms/step - loss: 2.2991 - accuracy: 0.4972 - val_loss: 2.9159 - val_accuracy: 0.3934\n","Epoch 12/30\n","1607/1607 [==============================] - 153s 95ms/step - loss: 2.1833 - accuracy: 0.5167 - val_loss: 3.6589 - val_accuracy: 0.3275\n","Epoch 13/30\n","1607/1607 [==============================] - 142s 88ms/step - loss: 2.1064 - accuracy: 0.5324 - val_loss: 3.6984 - val_accuracy: 0.3135\n","Epoch 14/30\n","1607/1607 [==============================] - 144s 89ms/step - loss: 2.0339 - accuracy: 0.5433 - val_loss: 4.8984 - val_accuracy: 0.2809\n","Epoch 15/30\n","1607/1607 [==============================] - 167s 104ms/step - loss: 1.9627 - accuracy: 0.5589 - val_loss: 2.8317 - val_accuracy: 0.4027\n","Epoch 16/30\n","1607/1607 [==============================] - 145s 90ms/step - loss: 1.8967 - accuracy: 0.5682 - val_loss: 2.7574 - val_accuracy: 0.4318\n","Epoch 17/30\n","1607/1607 [==============================] - 147s 91ms/step - loss: 1.8446 - accuracy: 0.5805 - val_loss: 2.7416 - val_accuracy: 0.4418\n","Epoch 18/30\n","1607/1607 [==============================] - 141s 88ms/step - loss: 1.7983 - accuracy: 0.5888 - val_loss: 2.7273 - val_accuracy: 0.4397\n","Epoch 19/30\n","1607/1607 [==============================] - 140s 87ms/step - loss: 1.7569 - accuracy: 0.5950 - val_loss: 2.3679 - val_accuracy: 0.4937\n","Epoch 20/30\n","1607/1607 [==============================] - 146s 91ms/step - loss: 1.7154 - accuracy: 0.6030 - val_loss: 2.6426 - val_accuracy: 0.4497\n","Epoch 21/30\n","1607/1607 [==============================] - 143s 89ms/step - loss: 1.6690 - accuracy: 0.6116 - val_loss: 3.5512 - val_accuracy: 0.3847\n","Epoch 22/30\n","1607/1607 [==============================] - 147s 91ms/step - loss: 1.6289 - accuracy: 0.6194 - val_loss: 2.9445 - val_accuracy: 0.4124\n","Epoch 23/30\n","1607/1607 [==============================] - 208s 129ms/step - loss: 1.5949 - accuracy: 0.6268 - val_loss: 2.4848 - val_accuracy: 0.4763\n","Epoch 24/30\n","1607/1607 [==============================] - 176s 109ms/step - loss: 1.5692 - accuracy: 0.6334 - val_loss: 3.0800 - val_accuracy: 0.4128\n","Epoch 25/30\n","1607/1607 [==============================] - 145s 90ms/step - loss: 1.5355 - accuracy: 0.6399 - val_loss: 2.8071 - val_accuracy: 0.4258\n","Epoch 26/30\n","1607/1607 [==============================] - 145s 90ms/step - loss: 1.5065 - accuracy: 0.6440 - val_loss: 2.7487 - val_accuracy: 0.4324\n","Epoch 27/30\n","1607/1607 [==============================] - 148s 92ms/step - loss: 1.4838 - accuracy: 0.6479 - val_loss: 3.8793 - val_accuracy: 0.3839\n","Epoch 28/30\n","1607/1607 [==============================] - 143s 89ms/step - loss: 1.4472 - accuracy: 0.6580 - val_loss: 2.7906 - val_accuracy: 0.4462\n","Epoch 29/30\n","1607/1607 [==============================] - 148s 92ms/step - loss: 1.4266 - accuracy: 0.6565 - val_loss: 2.5159 - val_accuracy: 0.4923\n","Epoch 30/30\n","1607/1607 [==============================] - 141s 88ms/step - loss: 1.4037 - accuracy: 0.6634 - val_loss: 2.6715 - val_accuracy: 0.4785\n","\n","accuracy:  0.4719691427811398\n"]}],"source":["num_epochs = 30\n","\n","model_cnn = models.Sequential()\n","model_cnn.add(layers.Input(shape=img_shape))\n","model_cnn.add(layers.Conv2D(128, 3, strides=2, padding=\"same\", activation='relu'))\n","model_cnn.add(layers.BatchNormalization())\n","model_cnn.add(layers.MaxPooling2D(3, strides=2, padding=\"same\"))\n","model_cnn.add(layers.Conv2D(128, 3, strides=2, padding=\"same\", activation='relu'))\n","model_cnn.add(layers.BatchNormalization())\n","model_cnn.add(layers.GlobalAveragePooling2D())\n","model_cnn.add(layers.Dense(num_classes, activation='softmax'))\n","\n","model_cnn.compile(\n","    optimizer = 'adam',\n","    loss = 'categorical_crossentropy',\n","    metrics = ['accuracy']\n",")\n","\n","model_cnn.fit(\n","    x = train_gen,\n","    epochs = num_epochs,\n","    validation_data = valid_gen,\n","    validation_steps = None,\n","    shuffle = False\n",")\n","\n","pred_cnn = model_cnn.predict(test_gen)\n","pred_cnn = np.argmax(pred_cnn, axis=1)\n","print('\\naccuracy: ', accuracy_score(test_gen.labels, pred_cnn))"]},{"cell_type":"markdown","metadata":{},"source":["This model's accuracy is considerably improved over the first sequential network. The accuracy is still less than half, but given there are 450 different target classes, this performance is perhaps to be expected of a network that simply consists of two convolutional layers in sequence. The model likely could have improved further with even more epochs, but 30 epochs already took a long time with a GPU. Either way, it seems a more complex network will be required to achieve a test accuracy over 80 or 90%."]},{"cell_type":"markdown","metadata":{},"source":["### Recurrent Neural Network with ConvLSTM1D\n","\n","`ConvLSTM1D` is a type of recurrent network layer available in Keras that combines both LSTM and convolution. The training time is extremely long, so the number of epochs was reduced to just 2."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-12-05T02:09:26.405824Z","iopub.status.busy":"2022-12-05T02:09:26.404924Z","iopub.status.idle":"2022-12-05T03:50:31.769488Z","shell.execute_reply":"2022-12-05T03:50:31.768248Z","shell.execute_reply.started":"2022-12-05T02:09:26.405783Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-12-05 02:09:26.750621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-05 02:09:26.871308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-05 02:09:26.872086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-05 02:09:26.873229: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-12-05 02:09:26.873538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-05 02:09:26.874251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-05 02:09:26.874901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-05 02:09:29.021112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-05 02:09:29.021976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-05 02:09:29.022643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-05 02:09:29.023243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","2022-12-05 02:09:29.878089: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n"]},{"name":"stderr","output_type":"stream","text":["2022-12-05 02:09:33.755933: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"]},{"name":"stdout","output_type":"stream","text":["1607/1607 [==============================] - 2912s 2s/step - loss: 5.9362 - accuracy: 0.0126 - val_loss: 6.0158 - val_accuracy: 0.0115\n","Epoch 2/2\n","1607/1607 [==============================] - 3000s 2s/step - loss: 5.4719 - accuracy: 0.0423 - val_loss: 6.0517 - val_accuracy: 0.0167\n","\n","accuracy:  0.01556161468378001\n"]}],"source":["num_epochs = 2\n","\n","model_rnn = models.Sequential()\n","model_rnn.add(layers.Input(shape=img_shape))\n","model_rnn.add(layers.Rescaling(1. / 255))\n","model_rnn.add(layers.ConvLSTM1D(128, 3, activation='relu'))\n","model_rnn.add(layers.BatchNormalization())\n","model_rnn.add(layers.Flatten())\n","model_rnn.add(layers.Dense(num_classes, activation = 'softmax'))\n","\n","model_rnn.compile(\n","    optimizer = 'adam',\n","    loss = 'categorical_crossentropy',\n","    metrics = ['accuracy']\n",")\n","\n","model_rnn.fit(\n","    x = train_gen,\n","    epochs = num_epochs,\n","    validation_data = valid_gen,\n","    validation_steps = None,\n","    shuffle = False\n",")\n","\n","pred_rnn = model_rnn.predict(test_gen)\n","pred_rnn = np.argmax(pred_rnn, axis = 1)\n","print('\\naccuracy: ', accuracy_score(test_gen.labels, pred_rnn))"]},{"cell_type":"markdown","metadata":{},"source":["The accuracy shown is pretty bad, but we think this is only due to having so few epochs to train the model. We think that if the model were to be given 30 epochs like the other models in this notebook, it will have done at least as well as the convolutional network, if not better."]},{"cell_type":"markdown","metadata":{},"source":["### Pretrained Model\n","\n","We will take a pretrained model and use transfer learning to apply the model to this particular task. This will jumpstart our model so that it can identify useful features in the data from the outset. Here, we will use the EfficientNetB3 model to make training slightly faster."]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-12-04T10:39:11.176780Z","iopub.status.busy":"2022-12-04T10:39:11.176411Z","iopub.status.idle":"2022-12-04T13:15:23.139706Z","shell.execute_reply":"2022-12-04T13:15:23.138639Z","shell.execute_reply.started":"2022-12-04T10:39:11.176747Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","1607/1607 [==============================] - 323s 194ms/step - loss: 3.0409 - accuracy: 0.3449 - val_loss: 1.3059 - val_accuracy: 0.6522\n","Epoch 2/30\n","1607/1607 [==============================] - 329s 205ms/step - loss: 1.3034 - accuracy: 0.6608 - val_loss: 0.8500 - val_accuracy: 0.7748\n","Epoch 3/30\n","1607/1607 [==============================] - 312s 194ms/step - loss: 0.9696 - accuracy: 0.7404 - val_loss: 0.7793 - val_accuracy: 0.7989\n","Epoch 4/30\n","1607/1607 [==============================] - 299s 186ms/step - loss: 0.8485 - accuracy: 0.7740 - val_loss: 0.6370 - val_accuracy: 0.8378\n","Epoch 5/30\n","1607/1607 [==============================] - 297s 185ms/step - loss: 0.7615 - accuracy: 0.7949 - val_loss: 0.5843 - val_accuracy: 0.8519\n","Epoch 6/30\n","1607/1607 [==============================] - 298s 186ms/step - loss: 0.6767 - accuracy: 0.8156 - val_loss: 0.6004 - val_accuracy: 0.8560\n","Epoch 7/30\n","1607/1607 [==============================] - 298s 185ms/step - loss: 0.6238 - accuracy: 0.8317 - val_loss: 0.5855 - val_accuracy: 0.8535\n","Epoch 8/30\n","1607/1607 [==============================] - 299s 186ms/step - loss: 0.5902 - accuracy: 0.8406 - val_loss: 0.5823 - val_accuracy: 0.8622\n","Epoch 9/30\n","1607/1607 [==============================] - 404s 251ms/step - loss: 0.5609 - accuracy: 0.8471 - val_loss: 0.5612 - val_accuracy: 0.8693\n","Epoch 10/30\n","1607/1607 [==============================] - 328s 204ms/step - loss: 0.5172 - accuracy: 0.8584 - val_loss: 0.5247 - val_accuracy: 0.8723\n","Epoch 11/30\n","1607/1607 [==============================] - 302s 188ms/step - loss: 0.4763 - accuracy: 0.8690 - val_loss: 0.5280 - val_accuracy: 0.8797\n","Epoch 12/30\n","1607/1607 [==============================] - 298s 186ms/step - loss: 0.4572 - accuracy: 0.8755 - val_loss: 0.5538 - val_accuracy: 0.8798\n","Epoch 13/30\n","1607/1607 [==============================] - 306s 190ms/step - loss: 0.4352 - accuracy: 0.8804 - val_loss: 0.5135 - val_accuracy: 0.8809\n","Epoch 14/30\n","1607/1607 [==============================] - 304s 189ms/step - loss: 0.4054 - accuracy: 0.8882 - val_loss: 0.4564 - val_accuracy: 0.8929\n","Epoch 15/30\n","1607/1607 [==============================] - 320s 199ms/step - loss: 0.3874 - accuracy: 0.8913 - val_loss: 0.4872 - val_accuracy: 0.8922\n","Epoch 16/30\n","1607/1607 [==============================] - 304s 189ms/step - loss: 0.3527 - accuracy: 0.9006 - val_loss: 0.4747 - val_accuracy: 0.8910\n","Epoch 17/30\n","1607/1607 [==============================] - 299s 186ms/step - loss: 0.3577 - accuracy: 0.8996 - val_loss: 0.4651 - val_accuracy: 0.8947\n","Epoch 18/30\n","1607/1607 [==============================] - 300s 187ms/step - loss: 0.3231 - accuracy: 0.9094 - val_loss: 0.4638 - val_accuracy: 0.8939\n","Epoch 19/30\n","1607/1607 [==============================] - 298s 185ms/step - loss: 0.3162 - accuracy: 0.9107 - val_loss: 0.4714 - val_accuracy: 0.9006\n","Epoch 20/30\n","1607/1607 [==============================] - 303s 189ms/step - loss: 0.2898 - accuracy: 0.9178 - val_loss: 0.4751 - val_accuracy: 0.8977\n","Epoch 21/30\n","1607/1607 [==============================] - 319s 198ms/step - loss: 0.2862 - accuracy: 0.9198 - val_loss: 0.5509 - val_accuracy: 0.8844\n","Epoch 22/30\n","1607/1607 [==============================] - 298s 185ms/step - loss: 0.2869 - accuracy: 0.9178 - val_loss: 0.4679 - val_accuracy: 0.9026\n","Epoch 23/30\n","1607/1607 [==============================] - 298s 185ms/step - loss: 0.2683 - accuracy: 0.9243 - val_loss: 0.4551 - val_accuracy: 0.9003\n","Epoch 24/30\n","1607/1607 [==============================] - 299s 186ms/step - loss: 0.2520 - accuracy: 0.9280 - val_loss: 0.4781 - val_accuracy: 0.9043\n","Epoch 25/30\n","1607/1607 [==============================] - 301s 187ms/step - loss: 0.2570 - accuracy: 0.9270 - val_loss: 0.4592 - val_accuracy: 0.9093\n","Epoch 26/30\n","1607/1607 [==============================] - 299s 186ms/step - loss: 0.2175 - accuracy: 0.9372 - val_loss: 0.4741 - val_accuracy: 0.9048\n","Epoch 27/30\n","1607/1607 [==============================] - 311s 194ms/step - loss: 0.2276 - accuracy: 0.9355 - val_loss: 0.4806 - val_accuracy: 0.9038\n","Epoch 28/30\n","1607/1607 [==============================] - 298s 186ms/step - loss: 0.2286 - accuracy: 0.9350 - val_loss: 0.4917 - val_accuracy: 0.9010\n","Epoch 29/30\n","1607/1607 [==============================] - 297s 185ms/step - loss: 0.2225 - accuracy: 0.9369 - val_loss: 0.4721 - val_accuracy: 0.9095\n","Epoch 30/30\n","1607/1607 [==============================] - 297s 185ms/step - loss: 0.2110 - accuracy: 0.9394 - val_loss: 0.4773 - val_accuracy: 0.9055\n","\n","accuracy:  0.9084258828223715\n"]}],"source":["from tensorflow.keras.applications.efficientnet import EfficientNetB3\n","\n","num_epochs = 30\n","\n","# init base model\n","base_model = EfficientNetB3(include_top=False, input_shape=img_shape, weights=\"imagenet\", pooling='max')\n","\n","# set layers past 100 to be untrainable\n","fine_tune_at = 100\n","for layer in base_model.layers[:fine_tune_at]:\n","  layer.trainable = False\n","\n","model_pre = models.Sequential()\n","model_pre.add(layers.Input(shape=img_shape))\n","model_pre.add(base_model)\n","model_pre.add(layers.BatchNormalization())\n","model_pre.add(layers.Dense(256, activation='relu'))\n","model_pre.add(layers.Dropout(rate=.2, seed=1234))\n","model_pre.add(layers.Dense(256, activation='relu'))\n","model_pre.add(layers.Dropout(rate=.2, seed=1234))\n","model_pre.add(layers.Dense(num_classes, activation='softmax'))\n","\n","model_pre.compile(\n","    optimizer = 'adam',\n","    loss = 'categorical_crossentropy',\n","    metrics = ['accuracy']\n",")\n","\n","model_pre.fit(\n","    x = train_gen,\n","    epochs = num_epochs,\n","    validation_data = valid_gen,\n","    validation_steps = None,\n","    shuffle = False\n",")\n","\n","pred_pre = model_pre.predict(test_gen)\n","pred_pre = np.argmax(pred_pre, axis=1)\n","print('\\naccuracy: ', accuracy_score(test_gen.labels, pred_pre))"]},{"cell_type":"markdown","metadata":{},"source":["As we can see, adding the pretrained model drastically improved the accuracy, as was to be expected."]},{"cell_type":"markdown","metadata":{},"source":["### Analysis\n","\n","As noted before, the sequential model's performance was extremely bad. This is likely because the network is far too simple to gleam any useful information from a given input. Given that the accuracy is very close to 0% and that the validation accuracy quickly plateaued, the model likely wasn't able to learn any useful information at all.\n","\n","The CNN was more promising than a simple dense sequential model. This is likely because CNNs are generally useful for learning information from image data. The CNN's poor performance in this notebook speaks more to the complexity of the data than to the power of CNNs. More epochs, more convolutional layers, and more complexity in the network would have all likely improved the performance of the model.\n","\n","It is hard to properly judge the performance of the RNN because we provided so little time for the model to properly train. The jump in accuracy between the two epochs was greater than what was demonstrated by the prior two models, indicating that at the very least, the RNN was learning more useful information more quickly. As stated before, we believe that with more epochs, this model would have performed at least as well as our CNN. Although RNNs are typically used for processing time-series data, we think that by combing LSTM with convolution, the network would be able to converge on a solution in a smaller number of iterations.\n","\n","And as expected, the model based on the pretrained model was by far the best. Pretrained models are already primed to extract useful information that can help to differentiate images. This gives the network a headstart towards learning information that improves performance on this specific task. Furthermore, by using the EfficientNet model, the overall training time was not much worse than that of the CNN. This demonstrates that for any image classification task, using a pretrained model can immediately save a lot of time and effort."]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.8 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"}}},"nbformat":4,"nbformat_minor":4}
